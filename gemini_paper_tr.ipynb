{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimin\\anaconda3\\envs\\Openai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    " \n",
    "import google.generativeai as genai\n",
    " \n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from config import Config\n",
    " \n",
    "# 서식이 지정된 Markdown 텍스트를 표시하는 함수\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    " \n",
    "# 제미나이 API 키 설정\n",
    "genai.configure(api_key=Config.GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0827\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')  # 텍스트 전용 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file 'Attention paper summary' as: https://generativelanguage.googleapis.com/v1beta/files/hfho6q07v6v2\n"
     ]
    }
   ],
   "source": [
    "# Path to the PDF file in the 'files' folder\n",
    "file_path = \"C:/Users/kimin/cv-project/my-cv-project/2017_NIPS_Attention Is All You Need.pdf\"\n",
    "\n",
    "sample_file = genai.upload_file(path=file_path, display_name=\"Attention paper summary\")\n",
    "\n",
    "# Confirm upload\n",
    "print(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main contribution of the paper \"Attention is All You Need\" is the introduction of the **Transformer** model. This architecture is significant because it **completely removes the need for recurrence and convolutions**, relying solely on an attention mechanism to draw global dependencies between input and output. This allows for:\n",
      "\n",
      "* **Increased parallelization:** Transformers can process sequences much faster than recurrent models, especially for long sequences.\n",
      "* **Improved performance:** The Transformer achieves state-of-the-art results in machine translation tasks, surpassing even ensemble models that combine multiple recurrent architectures. \n",
      "* **Better long-range dependency learning:** The attention mechanism allows the model to directly consider relationships between distant words in a sentence, potentially improving performance on tasks requiring this capability.\n",
      "\n",
      "The paper also provides a detailed analysis of different attention mechanisms, proposes multi-head attention for attending to information from different representation subspaces, and introduces a novel positional encoding scheme. All of these innovations contributed to the Transformer's significant impact on the field of natural language processing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate content using the uploaded document\n",
    "response = model.generate_content([sample_file, \"What is main contribution in paper?\"])\n",
    "\n",
    "# Print the generated content\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
